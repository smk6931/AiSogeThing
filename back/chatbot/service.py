from core.database import fetch_all
from client.openai_client import get_embedding_openai, generate_response_openai
from utils.safe_ops import safe_execute

async def analyze_user_context(user_id: int) -> str:
    """
    ìœ ì €ì˜ ì‹œì²­ ê¸°ë¡ê³¼ êµ¬ë… ì±„ë„ì„ ê¸°ë°˜ìœ¼ë¡œ 'Context String' ìƒì„±
    """
    # 1. ìµœê·¼ ì‹œì²­ ê¸°ë¡ (ìƒìœ„ 10ê°œ)
    log_sql = """
        SELECT yl.title, yl.channel_title, yl.tags
        FROM user_youtube_logs uyl
        JOIN youtube_list yl ON uyl.video_id = yl.video_id
        WHERE uyl.user_id = :uid
        ORDER BY uyl.updated_at DESC
        LIMIT 100
    """
    logs = await fetch_all(log_sql, {"uid": user_id})
    
    # 2. êµ¬ë… ì±„ë„ (ìƒìœ„ 5ê°œ)
    sub_sql = """
        SELECT yc.name, yc.keywords 
        FROM user_logs ul
        JOIN youtube_channels yc ON ul.content_id = yc.channel_id
        WHERE ul.user_id = :uid AND ul.action = 'subscribe'
        ORDER BY ul.created_at DESC
        LIMIT 100
    """
    subs = await fetch_all(sub_sql, {"uid": user_id})
    
    # Context ìš”ì•½
    context_lines = []
    if logs:
        titles = [f"'{row['title']}'" for row in logs]
        context_lines.append(f"ìµœê·¼ ì‹œì²­ ì˜ìƒ: {', '.join(titles)}")
    
    if subs:
        channels = [f"{row['name']}" for row in subs]
        context_lines.append(f"êµ¬ë… ì±„ë„: {', '.join(channels)}")
        
    return "\n".join(context_lines) if context_lines else "ì‹ ê·œ ìœ ì € (ì‹œì²­ ê¸°ë¡ ì—†ìŒ)"

async def retrieve_videos(query_text: str, limit: int = 4):
    """
    [RAG] ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì˜ìƒì„ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ê¸°
    """
    # 1. ì§ˆë¬¸ ë²¡í„°í™”
    query_vector = await get_embedding_openai(query_text)
    
    # 2. ë²¡í„° ê²€ìƒ‰ (Cosine Distance: <=>)
    sql = """
        SELECT video_id, title, channel_title, description, 
               (1 - (embedding <=> :qv)) as similarity
        FROM youtube_list
        WHERE embedding IS NOT NULL
        ORDER BY embedding <=> :qv ASC
        LIMIT :limit
    """
    
    try:
        results = await fetch_all(sql, {"qv": str(query_vector), "limit": limit})
        return results
    except Exception as e:
        print(f"âš ï¸ Vector Search Error: {e}")
        return []

async def process_chat(user_id: int, message: str) -> str:
    """
    [Main Logic] ì±—ë´‡ ëŒ€í™” ì²˜ë¦¬ (RAG + LLM)
    """
    print(f"\nğŸ’¬ [ChatRequest] User: {user_id}, Message: {message}")
    
    # 1. ìœ ì € ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ (ë‚´ ì·¨í–¥)
    user_context = await analyze_user_context(user_id)
    print(f"ğŸ‘¤ [Context] User Profile Loaded ({len(user_context)} chars)")
    
    # 2. ì§ˆë¬¸ ì˜ë„ íŒŒì•…: ë²¡í„° ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨
    # ë©”íƒ€ ì§ˆë¬¸ í‚¤ì›Œë“œ (ì„±í–¥ ë¶„ì„, ì„œë¹„ìŠ¤ ì•ˆë‚´ ë“±)
    meta_keywords = ["ì„±í–¥", "ë¶„ì„", "ì•ˆë‚´", "ì„¤ëª…", "ì‚¬ìš©ë²•", "ê¸°ëŠ¥", "ì„œë¹„ìŠ¤", "ì·¨í–¥"]
    is_meta_question = any(keyword in message for keyword in meta_keywords)
    
    video_context = ""
    if not is_meta_question:
        # 3-A. ì½˜í…ì¸  ì§ˆë¬¸ â†’ ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
        print("ğŸ” [Intent] Content query detected. Running vector search...")
        relevant_videos = await retrieve_videos(message)
        print(f"ğŸ” [VectorSearch] Found {len(relevant_videos)} relevant videos.")
        
        if relevant_videos:
            video_infos = []
            for v in relevant_videos:
                sim = v['similarity']
                info = f"- [{v['title']}] ({v['channel_title']}): ìœ ì‚¬ë„ {sim:.3f}"
                video_infos.append(info)
                print(f"   ğŸ‘‰ {info} (ID: {v['video_id']})")
                
            video_context = "ê´€ë ¨ ì˜ìƒ DB ê²€ìƒ‰ ê²°ê³¼:\n" + "\n".join(video_infos)
    else:
        # 3-B. ë©”íƒ€ ì§ˆë¬¸ â†’ ë²¡í„° ê²€ìƒ‰ ìŠ¤í‚µ
        print("ğŸ§  [Intent] Meta question detected. Skipping vector search (using user context only).")
    
    # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """
    ë‹¹ì‹ ì€ 'AiSogeThing'ì˜ AI íë ˆì´í„°ì…ë‹ˆë‹¤.
    ìœ ì €ì˜ ì‹œì²­ ê¸°ë¡ê³¼ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬, ê°€ì¥ ì ì ˆí•œ ë‹µë³€ê³¼ ì˜ìƒ ì¶”ì²œì„ ì œê³µí•˜ì„¸ìš”.
    - ë§íˆ¬: ì¹œê·¼í•˜ê³  ìœ„íŠ¸ ìˆê²Œ (ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš©)
    - í˜•ì‹: Markdown ë¬¸ë²• í™œìš© (ë³¼ë“œì²´, ë¦¬ìŠ¤íŠ¸ ë“±)
    - ì„±í–¥ ë¶„ì„: User Profile ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ MBTIì²˜ëŸ¼ ì¬ë¯¸ìˆê²Œ ë¶„ì„í•˜ì„¸ìš”.
    - ì¶”ì²œ ì‹œ: ì˜ìƒ ì œëª©, ì±„ë„ëª…ì„ ëª…í™•íˆ ì–¸ê¸‰í•˜ê³ , DB ê²€ìƒ‰ ê²°ê³¼ì— ìˆëŠ” ì˜ìƒë§Œ ì¶”ì²œí•˜ì„¸ìš”.
    - ë°ì´í„° ì¶œì²˜: ë°˜ë“œì‹œ ì œê³µëœ [User Profile] ë˜ëŠ” [DB Context] ë‚´ì—ì„œë§Œ ë‹µë³€í•˜ê³ , ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
    """
    
    final_prompt = f"""
    [User Profile]
    {user_context}
    
    [DB Context (RAG)]
    {video_context if video_context else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (User Profile ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€)"}
    
    [User Message]
    {message}
    """
    
    # 5. LLM ë‹µë³€ ìƒì„±
    print("ğŸ¤– [LLM] Generating response via OpenAI...")
    response = await generate_response_openai(final_prompt, system_role=system_prompt)
    print("âœ… [LLM] Response Generated.\n")
    return response
